"""
Example improved chunking implementation.
This shows how to enhance the current hierarchical parser with:
1. Size limits
2. Overlap
3. Token-aware chunking
4. Smart splitting

To use: Copy relevant parts into embedding_processor.py
"""

import os
import re
import logging
from typing import List, Tuple
from langchain_core.documents import Document
import datetime

# Try to import tiktoken for token counting
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False
    logging.warning("tiktoken not available. Using character-based limits.")

logger = logging.getLogger(__name__)

# Configuration constants
MAX_CHUNK_SIZE = int(os.getenv("MAX_CHUNK_SIZE", "2000"))  # Characters
MAX_CHUNK_TOKENS = int(os.getenv("MAX_CHUNK_TOKENS", "500"))  # Tokens
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "200"))  # Characters
SPLIT_LONG_PARAGRAPHS = os.getenv("SPLIT_LONG_PARAGRAPHS", "true").lower() == "true"

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """
    Count tokens in text using tiktoken.
    Falls back to character-based estimation if tiktoken unavailable.
    """
    if TIKTOKEN_AVAILABLE:
        try:
            encoding = tiktoken.encoding_for_model(model)
            return len(encoding.encode(text))
        except Exception as e:
            logger.warning(f"Token counting failed: {e}. Using character estimate.")
    
    # Fallback: rough estimate (1 token â‰ˆ 4 characters)
    return len(text) // 4

def split_text_by_sentences(text: str) -> List[str]:
    """Split text into sentences, preserving punctuation."""
    # Simple sentence splitting (can be improved with nltk or spacy)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if s.strip()]

def create_overlapping_chunks(
    text: str,
    max_size: int,
    overlap: int,
    h1: str,
    h2: str,
    h3: str,
    h4: str,
    meta: dict
) -> List[Document]:
    """
    Split long text into overlapping chunks.
    Tries to split at sentence boundaries when possible.
    """
    chunks = []
    
    # First, try sentence-based splitting
    sentences = split_text_by_sentences(text)
    
    current_chunk_sentences = []
    current_size = 0
    
    for sentence in sentences:
        sentence_size = len(sentence)
        
        # If adding this sentence would exceed limit, finalize current chunk
        if current_size + sentence_size > max_size and current_chunk_sentences:
            # Create chunk from accumulated sentences
            chunk_text = " ".join(current_chunk_sentences)
            chunk = create_single_chunk(chunk_text, h1, h2, h3, h4, meta)
            chunks.append(chunk)
            
            # Start new chunk with overlap
            # Take last N sentences for overlap
            overlap_sentences = []
            overlap_size = 0
            for s in reversed(current_chunk_sentences):
                if overlap_size + len(s) <= overlap:
                    overlap_sentences.insert(0, s)
                    overlap_size += len(s)
                else:
                    break
            
            current_chunk_sentences = overlap_sentences
            current_size = overlap_size
        
        current_chunk_sentences.append(sentence)
        current_size += sentence_size + 1  # +1 for space
    
    # Add final chunk
    if current_chunk_sentences:
        chunk_text = " ".join(current_chunk_sentences)
        chunk = create_single_chunk(chunk_text, h1, h2, h3, h4, meta)
        chunks.append(chunk)
    
    return chunks

def create_single_chunk(
    text: str,
    h1: str,
    h2: str,
    h3: str,
    h4: str,
    meta: dict
) -> Document:
    """Create a single document chunk with hierarchical headers."""
    # Create prepended header
    prepended_text_parts = []
    if h1: prepended_text_parts.append(f"Title: {h1}")
    if h2: prepended_text_parts.append(f"Section: {h2}")
    if h3: prepended_text_parts.append(f"Subsection: {h3}")
    if h4: prepended_text_parts.append(f"Sub-subsection: {h4}")
    
    prepended_header = "\n".join(prepended_text_parts)
    final_page_content = f"{prepended_header}\n\n{text}" if prepended_header else text
    
    # Create metadata
    final_metadata = meta.copy()
    if h1: final_metadata['doc_title_hierarchical'] = h1
    if h2: final_metadata['section_title'] = h2
    if h3: final_metadata['subsection_title'] = h3
    if h4: final_metadata['subsubsection_title'] = h4
    final_metadata['chunk_type'] = 'section' if h2 or h3 or h4 else 'text'
    final_metadata['content_length'] = len(text)
    final_metadata['token_count'] = count_tokens(text)
    
    # Clean up date objects
    for key, value in final_metadata.items():
        if isinstance(value, datetime.date) and not isinstance(value, datetime.datetime):
            final_metadata[key] = datetime.datetime.combine(value, datetime.time.min)
    
    return Document(page_content=final_page_content, metadata=final_metadata)

def create_document_chunk_improved(
    paragraph_lines_list: List[str],
    h1: str,
    h2: str,
    h3: str,
    h4: str,
    meta: dict
) -> List[Document]:
    """
    Improved version of create_document_chunk with size limits and overlap.
    
    This replaces the original create_document_chunk function.
    """
    if not paragraph_lines_list:
        return []
    
    text = "\n".join(paragraph_lines_list).strip()
    if not text:
        return []
    
    # Check if chunk needs splitting
    text_size = len(text)
    text_tokens = count_tokens(text)
    
    needs_splitting = False
    if SPLIT_LONG_PARAGRAPHS:
        # Check both character and token limits
        if text_size > MAX_CHUNK_SIZE or text_tokens > MAX_CHUNK_TOKENS:
            needs_splitting = True
    
    if needs_splitting:
        logger.debug(
            f"Splitting large chunk: {text_size} chars, {text_tokens} tokens. "
            f"Section: {h2 or h3 or h4 or 'Unknown'}"
        )
        # Use overlapping chunking
        return create_overlapping_chunks(
            text,
            max_size=MAX_CHUNK_SIZE,
            overlap=CHUNK_OVERLAP,
            h1=h1,
            h2=h2,
            h3=h3,
            h4=h4,
            meta=meta
        )
    else:
        # Create single chunk (original behavior)
        return [create_single_chunk(text, h1, h2, h3, h4, meta)]


# Example usage in parse_markdown_hierarchically:
# Replace the create_document_chunk call with:
# chunks.extend(create_document_chunk_improved(
#     current_paragraph_lines, current_h1, current_h2, current_h3, current_h4, current_metadata
# ))

