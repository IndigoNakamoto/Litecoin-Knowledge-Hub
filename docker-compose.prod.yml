version: '3.8'

services:
  # MongoDB service (local)
  # Credentials are injected via docker-compose.override.yml from .env.secrets.
  mongodb:
    image: mongo:7.0
    container_name: litecoin-mongodb
    # Only enable --auth if MONGO_INITDB_ROOT_PASSWORD is set
    # --bind_ip_all allows connections from other containers
    entrypoint: >
      sh -c "
      if [ -n \"$$MONGO_INITDB_ROOT_PASSWORD\" ]; then
        export MONGO_INITDB_ROOT_USERNAME=\"$${MONGO_INITDB_ROOT_USERNAME:-admin}\";
        export MONGO_INITDB_ROOT_PASSWORD=\"$$MONGO_INITDB_ROOT_PASSWORD\";
      else
        unset MONGO_INITDB_ROOT_USERNAME;
        unset MONGO_INITDB_ROOT_PASSWORD;
      fi;
      exec docker-entrypoint.sh mongod --bind_ip_all $${MONGO_INITDB_ROOT_PASSWORD:+--auth}
      "
    volumes:
      - mongodb_data:/data/db
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "if [ -n \"$$MONGO_INITDB_ROOT_PASSWORD\" ]; then mongosh --quiet --username $$MONGO_INITDB_ROOT_USERNAME --password $$MONGO_INITDB_ROOT_PASSWORD --authenticationDatabase admin --eval \"db.adminCommand('ping')\"; else mongosh --quiet --eval \"db.adminCommand('ping')\"; fi"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: litecoin-backend
    ports:
      - "8000:8000"
    env_file:
      - ./.env.docker.prod
      - ./backend/.env
    environment:
      # Production-specific overrides
      # Note: GOOGLE_API_KEY is loaded from ./backend/.env via env_file above
      - NODE_ENV=production
      # Fix gRPC/asyncio compatibility issue on Python 3.11+ in Linux containers
      - GRPC_POLL_STRATEGY=epoll1
      # Internal Docker service URL for Payload CMS (for suggested questions cache)
      - PAYLOAD_URL=http://payload_cms:3000
      # Disable HTTPS redirect middleware when behind Cloudflare (Cloudflare handles redirects at edge)
      - BEHIND_CLOUDFLARE=true
      # CORS origins - defaults include chat.lite.space
      # Add admin frontend URL via CORS_ORIGINS or ADMIN_FRONTEND_URL when running admin locally
      # Example: CORS_ORIGINS=https://chat.lite.space,http://localhost:3003
      # Or set: ADMIN_FRONTEND_URL=http://localhost:3003 (automatically added to CORS)
      - CORS_ORIGINS=${CORS_ORIGINS}
      # =============================================================================
      # Local RAG Configuration (optional - enable via feature flags)
      # =============================================================================
      # Service URLs
      # - Ollama: Docker container (ollama:11434)
      # - Infinity: Native on host for Apple Silicon (host.docker.internal:7997)
      #   or Docker container on x86_64 (infinity:7997)
      # - Redis Stack: Docker container (redis_stack:6380)
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - INFINITY_URL=${INFINITY_URL:-http://host.docker.internal:7997}
      - REDIS_STACK_URL=${REDIS_STACK_URL:-redis://redis_stack:6380}
      # Feature flags (set to 'true' to enable local RAG)
      - USE_LOCAL_REWRITER=${USE_LOCAL_REWRITER:-false}
      - USE_INFINITY_EMBEDDINGS=${USE_INFINITY_EMBEDDINGS:-false}
      - USE_REDIS_CACHE=${USE_REDIS_CACHE:-false}
      # Router configuration
      - MAX_LOCAL_QUEUE_DEPTH=${MAX_LOCAL_QUEUE_DEPTH:-3}
      - LOCAL_TIMEOUT_SECONDS=${LOCAL_TIMEOUT_SECONDS:-2.0}
      # Model configuration
      - LOCAL_REWRITER_MODEL=${LOCAL_REWRITER_MODEL:-llama3.2:3b}
      - EMBEDDING_MODEL_ID=${EMBEDDING_MODEL_ID:-BAAI/bge-m3}
      - VECTOR_DIMENSION=${VECTOR_DIMENSION:-1024}
      # Redis Stack cache configuration
      - REDIS_CACHE_INDEX_NAME=${REDIS_CACHE_INDEX_NAME:-cache:index}
      - REDIS_CACHE_SIMILARITY_THRESHOLD=${REDIS_CACHE_SIMILARITY_THRESHOLD:-0.90}
    depends_on:
      mongodb:
        condition: service_healthy
      payload_cms:
        condition: service_healthy
    volumes:
      - ./backend/monitoring/data:/app/backend/monitoring/data
      - faiss_index_data:/app/backend/faiss_index_1024  # Persist FAISS index between restarts
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  payload_cms:
    build:
      context: ./payload_cms
      dockerfile: Dockerfile
    container_name: litecoin-payload-cms
    ports:
      - "3001:3000"
    env_file:
      - ./.env.docker.prod
      - ./payload_cms/.env
    environment:
      # Production-specific overrides
      # Note: PAYLOAD_SECRET is loaded from ./payload_cms/.env via env_file above
      - NODE_ENV=production
    depends_on:
      mongodb:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3000', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL:-https://api.lite.space}
        - NEXT_PUBLIC_PAYLOAD_URL=${NEXT_PUBLIC_PAYLOAD_URL:-https://cms.lite.space}
    container_name: litecoin-frontend
    ports:
      - "3000:3000"
    env_file:
      - ./.env.docker.prod
    environment:
      # Production-specific overrides
      - NODE_ENV=production
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "const rawHost = process.env.HOST || process.env.HOSTNAME || '127.0.0.1'; const host = ['0.0.0.0', '::', '::0', '::ffff:0:0'].includes(rawHost) ? '127.0.0.1' : rawHost; require('http').get(`http://${host}:3000`, (r) => {process.exit(r.statusCode === 200 ? 0 : 1)}).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  prometheus:
    image: prom/prometheus:latest
    container_name: litecoin-prometheus
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy

  grafana:
    image: grafana/grafana:latest
    container_name: litecoin-grafana
    ports:
      - "127.0.0.1:3002:3000"
    env_file:
      - ./.env.docker.prod
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD must be set}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_started

  admin_frontend:
    build:
      context: ./admin-frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL:-https://api.lite.space}
    container_name: litecoin-admin-frontend
    ports:
      - "3003:3000"
    env_file:
      - ./.env.docker.prod
    environment:
      # Production-specific overrides
      - NODE_ENV=production
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "const rawHost = process.env.HOST || process.env.HOSTNAME || '127.0.0.1'; const host = ['0.0.0.0', '::', '::0', '::ffff:0:0'].includes(rawHost) ? '127.0.0.1' : rawHost; require('http').get(`http://${host}:3000`, (r) => {process.exit(r.statusCode === 200 ? 0 : 1)}).on('error', () => process.exit(1))"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: litecoin-cloudflared
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}
    env_file:
      - ./.env.docker.prod
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    depends_on:
      backend:
        condition: service_healthy
      payload_cms:
        condition: service_healthy
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: litecoin-redis
    env_file:
      - ./.env.docker.prod
    # Only add --requirepass if REDIS_PASSWORD is set
    command: >
      sh -c "if [ -n \"$$REDIS_PASSWORD\" ]; then
        redis-server --requirepass $$REDIS_PASSWORD --save 60 1 --loglevel warning;
      else
        redis-server --save 60 1 --loglevel warning;
      fi"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  # =============================================================================
  # LOCAL RAG SERVICES (High-Performance Local RAG with Cloud Spillover)
  # =============================================================================
  # These services enable local-first query processing with automatic cloud fallback.
  # See docs/features/DEC6_FEATURE_HIGH_PERFORMANCE_LOCAL_RAG.md for full documentation.
  #
  # To enable: Set USE_LOCAL_REWRITER=true, USE_INFINITY_EMBEDDINGS=true, USE_REDIS_CACHE=true
  # in your environment file.
  # =============================================================================

  # Redis Stack - Vector search cache with HNSW index for semantic caching
  # Replaces basic Redis when USE_REDIS_CACHE=true
  # Uses redis-stack-server which includes RediSearch, ReJSON modules
  redis_stack:
    image: redis/redis-stack-server:latest
    container_name: litecoin-redis-stack
    env_file:
      - ./.env.docker.prod
    environment:
      # Configure Redis Stack via environment variables (preserves module loading)
      - REDIS_ARGS=--maxmemory 4gb --maxmemory-policy allkeys-lfu --appendonly no --loglevel warning ${REDIS_PASSWORD:+--requirepass ${REDIS_PASSWORD}}
    ports:
      - "127.0.0.1:6380:6379"
    volumes:
      - redis_stack_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "${REDIS_PASSWORD:+-a}", "${REDIS_PASSWORD:-}", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    profiles:
      - local-rag

  # Ollama - Local LLM for query rewriting (llama3.2:3b)
  # Used when USE_LOCAL_REWRITER=true and queue depth < MAX_LOCAL_QUEUE_DEPTH
  ollama:
    image: ollama/ollama:latest
    container_name: litecoin-ollama
    ports:
      - "127.0.0.1:11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles:
      - local-rag

  # Infinity - Local embedding service (stella_en_1.5B_v5, 1024-dim)
  # Used when USE_INFINITY_EMBEDDINGS=true
  # Note: Running under Rosetta on Apple Silicon - memory usage is higher than native
  infinity:
    image: michaelf34/infinity:latest
    container_name: litecoin-infinity
    command: v2 --model-id dunzhang/stella_en_1.5B_v5 --port 7997 --device cpu
    ports:
      - "127.0.0.1:7997:7997"
    volumes:
      - infinity_model_cache:/app/.cache
    # No memory limit - Rosetta emulation needs more RAM for 1.5B model
    restart: "no"
    # Disable healthcheck - the image doesn't have curl and model loading is slow
    healthcheck:
      disable: true
    profiles:
      - local-rag

volumes:
  prometheus_data:
  grafana_data:
  mongodb_data:
  redis_data:
  faiss_index_data:  # Persist FAISS index between container restarts
  # Local RAG volumes
  redis_stack_data:
  ollama_data:
  infinity_model_cache:

